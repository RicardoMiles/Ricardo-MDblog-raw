# 写论文收尾阶段的三个工具

## 统计某个github仓库& 文件夹里面所有文件代码行数

有一个简单的方法可以统计 GitHub 仓库中所有文件的代码行数，主要依赖一些工具，比如 `cloc` 或 `tokei`。以下是实现的方法：

------

### 方法 1：使用 `cloc`

1. **安装 `cloc`**

   - 如果你使用的是 Windows，可以通过 `choco install cloc` 安装。
   - 在 macOS 上，使用 `brew install cloc`。
   - 在 Linux 上，使用包管理器，例如 `apt install cloc`。

2. **克隆仓库**

   ```bash
   git clone https://github.com/your-repo.git
   cd your-repo
   ```

3. **运行统计命令**

   ```bash
   cloc .
   ```

`cloc` 会统计每种语言的代码行数、注释行数和空白行数，生成一个详细的表格。

------

### 方法 2：使用 `tokei`

1. **安装 `tokei`**

   - Windows：通过 `scoop install tokei` 安装。
   - macOS：通过 `brew install tokei` 安装。
   - Linux：通过 `apt install tokei` 或从 [tokei GitHub 仓库](https://github.com/XAMPPRocky/tokei) 下载并安装。

2. **克隆仓库并统计**

   ```bash
   git clone https://github.com/your-repo.git
   cd your-repo
   tokei .
   ```

`tokei` 会列出所有文件类型的代码行数、注释行数和空白行数。

------

### 方法 3：手动统计（简单的 Shell 脚本）

如果你不想安装额外工具，也可以使用 `find` 和 `wc` 组合来统计：

```bash
git clone https://github.com/your-repo.git
cd your-repo

# 统计所有 .c, .cpp, .py, 等文件行数
find . -type f \( -name "*.c" -o -name "*.cpp" -o -name "*.py" -o -name "*.js" \) \
  | xargs wc -l
```

这将列出每个文件的行数以及总行数。

------

### 输出结果示例

以 `cloc` 的结果为例：

```plaintext
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Jupyter Notebook                76              0          20602           8321
Markdown                         1             38              0            149
-------------------------------------------------------------------------------
SUM:                            77             38          20602           8470
-------------------------------------------------------------------------------
```

`cloc` 默认情况下统计 Jupyter Notebook 文件（`.ipynb`）时，是 **包含输出单元格** 的内容的，因为 `.ipynb` 文件是以 JSON 格式存储的，其中不仅包括代码单元格，还包括输出单元格、Markdown 单元格等内容。

------

### 如何确认是否统计了输出？

1. **典型现象**：
   - 如果你的 Jupyter Notebook 中有大量的图像或长文本输出，那么注释行（`comment`）的数量可能会很高，而代码行（`code`）的数量会较低。这是因为 `cloc` 将 Notebook 中的非代码单元格（如 Markdown 或输出）归为“注释”。
2. **当前输出分析**：
   - **注释行数（`comment`）：20602**：这个数值非常高，说明 `cloc` 可能将 Notebook 中的输出单元格和 Markdown 单元格都算作了注释行。
   - **代码行数（`code`）：8321**：这反映了 Notebook 中实际的代码部分。



## 所有文件使用过的第三方库

但是他没有tensorflow keras什么的，我明明用了的还有kttuner

如果您的脚本没有捕捉到像 `tensorflow`、`keras` 或 `ktuner` 这样的库，可能是因为这些库的导入方式与简单的 `import` 或 `from ... import` 语句不同，或者它们被导入的方式使得正则表达式无法匹配。例如，有些代码可能会使用别名或者动态导入。

为了更准确地捕捉这些库，我们可以调整正则表达式以匹配更多种类的导入语句，包括那些使用别名的导入。此外，我们也可以检查是否有使用 `importlib` 进行动态导入的情况。

以下是更新后的脚本，它包括了更广泛的正则表达式来匹配不同的导入模式，并添加了对动态导入的基本检查：

```python
import os
import re
import json

def find_libraries_in_cell(cell):
    libraries = set()
    # 匹配 import 语句，包括带别名的
    import_pattern = re.compile(r'^\s*import\s+([\w\.]+)(?:\s+as\s+[\w]+)?')
    # 匹配 from ... import ... 语句，包括带别名的
    from_pattern = re.compile(r'^\s*from\s+([\w\.]+)(?:\s+import)?(?:\s+[\w\.,\s]+(?:\s+as\s+[\w]+)?)?')
    # 匹配 importlib 动态导入
    importlib_pattern = re.compile(r'importlib\.import_module\(([\'"][\w\.]+[\'"])\)')

    if isinstance(cell, list) and cell:
        for line in cell[0].split('\n'):
            import_match = import_pattern.match(line)
            if import_match:
                libraries.add(import_match.group(1))
            from_match = from_pattern.match(line)
            if from_match:
                libraries.add(from_match.group(1))
            importlib_match = importlib_pattern.search(line)
            if importlib_match:
                libraries.add(importlib_match.group(1).strip("'").strip('"'))

    return libraries

def summarize_libraries(folder_path):
    all_libraries = set()
    
    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            if filename.endswith('.ipynb'):
                ipynb_path = os.path.join(root, filename)
                print(f"Processing file: {ipynb_path}")
                try:
                    with open(ipynb_path, 'r', encoding='utf-8') as f:
                        notebook = json.load(f)
                        for cell in notebook['cells']:
                            if cell['cell_type'] == 'code' and 'source' in cell and isinstance(cell['source'], list):
                                cell_libraries = find_libraries_in_cell(cell['source'])
                                all_libraries.update(cell_libraries)
                except Exception as e:
                    print(f"Error reading {ipynb_path}: {e}")
    
    return all_libraries

# 获取当前工作目录的路径
folder_path = os.getcwd()
print(f"Searching for .ipynb files in: {folder_path}")
all_libraries = summarize_libraries(folder_path)

# 打印所有唯一的第三方库
print("Used libraries:")
for lib in sorted(all_libraries):
    print(lib)
```

这个脚本现在应该能够捕捉到更多的导入模式，包括那些使用别名的导入和动态导入的情况。如果您的 `.ipynb` 文件中使用了 `importlib` 来动态导入库，这个脚本也能够识别它们。再次运行这个脚本，看看是否能够捕捉到 `tensorflow`、`keras` 和 `ktuner` 等库。

```python
import os
import re
import json

def find_libraries_in_cell(cell):
    libraries = set()
    import_pattern = re.compile(r'^\s*import\s+([\w\.]+)(?:\s+as\s+[\w]+)?')
    from_pattern = re.compile(r'^\s*from\s+([\w\.]+)(?:\s+import)?(?:\s+[\w\.,\s]+(?:\s+as\s+[\w]+)?)?')
    importlib_pattern = re.compile(r'importlib\.import_module\(([\'"][\w\.]+[\'"])\)')

    if isinstance(cell, list) and cell:
        for line in cell[0].split('\n'):
            import_match = import_pattern.match(line)
            if import_match:
                libraries.add(import_match.group(1))
            from_match = from_pattern.match(line)
            if from_match:
                libraries.add(from_match.group(1))
            importlib_match = importlib_pattern.search(line)
            if importlib_match:
                libraries.add(importlib_match.group(1).strip("'").strip('"'))
            # 检查 keras-tuner 的导入
            if 'ktuner' in line or 'keras-tuner' in line:
                libraries.add('keras-tuner')

    return libraries

def summarize_libraries(folder_path):
    all_libraries = set()
    
    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            if filename.endswith('.ipynb'):
                ipynb_path = os.path.join(root, filename)
                print(f"Processing file: {ipynb_path}")
                try:
                    with open(ipynb_path, 'r', encoding='utf-8') as f:
                        notebook = json.load(f)
                        for cell in notebook['cells']:
                            if cell['cell_type'] == 'code' and 'source' in cell and isinstance(cell['source'], list):
                                cell_libraries = find_libraries_in_cell(cell['source'])
                                all_libraries.update(cell_libraries)
                except Exception as e:
                    print(f"Error reading {ipynb_path}: {e}")
    
    return all_libraries

# 获取当前工作目录的路径
folder_path = os.getcwd()
print(f"Searching for .ipynb files in: {folder_path}")
all_libraries = summarize_libraries(folder_path)

# 打印所有唯一的第三方库
print("Used libraries:")
for lib in sorted(all_libraries):
    print(lib)
```

### 获取被导入的库的函数功能调用

```python
import os
import ast
import json

class FunctionVisitor(ast.NodeVisitor):
    def __init__(self):
        self.libraries_usage = {}

    def visit_Call(self, node):
        if isinstance(node.func, ast.Attribute):
            lib_name = node.func.value.id
            func_name = node.func.attr
            if lib_name not in self.libraries_usage:
                self.libraries_usage[lib_name] = set()
            self.libraries_usage[lib_name].add(func_name)
        elif isinstance(node.func, ast.Name):
            if node.func.id not in self.libraries_usage:
                self.libraries_usage[node.func.id] = set()
            self.libraries_usage[node.func.id].add(node.func.id)
        self.generic_visit(node)

def summarize_usage(folder_path):
    libraries_usage = {}
    
    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            if filename.endswith('.ipynb'):
                ipynb_path = os.path.join(root, filename)
                print(f"Processing file: {ipynb_path}")
                try:
                    with open(ipynb_path, 'r', encoding='utf-8') as f:
                        notebook = json.load(f)
                        visitor = FunctionVisitor()
                        for cell in notebook['cells']:
                            if cell['cell_type'] == 'code' and 'source' in cell and isinstance(cell['source'], list):
                                for source in cell['source']:
                                    tree = ast.parse(source, filename, 'exec')
                                    visitor.visit(tree)
                                    libraries_usage.update(visitor.libraries_usage)
                except Exception as e:
                    print(f"Error reading {ipynb_path}: {e}")
    
    return libraries_usage

# 获取当前工作目录的路径
folder_path = os.getcwd()
print(f"Searching for .ipynb files in: {folder_path}")
libraries_usage = summarize_usage(folder_path)

# 打印库的使用情况
for library, functions in libraries_usage.items():
    print(f"Library: {library}")
    for func in sorted(functions):
        print(f"  Function: {func}")
    print()
```

## 从文件夹中的多个代码文件跨文件搜索字段

```python
import os

def search_variable(start_path, variable_name):
    for root, dirs, files in os.walk(start_path):
        for file in files:
            file_path = os.path.join(root, file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if variable_name in content:
                        print(f"变量 '{variable_name}' 在文件 {file_path} 中找到。")
            except Exception as e:
                print(f"无法读取文件 {file_path}：{e}")

# 使用示例
start_path = '/path/to/your/folder'  # 替换为你的项目文件夹路径
variable_name = 'your_variable_name'  # 替换为你想搜索的变量名
search_variable(start_path, variable_name)
```

